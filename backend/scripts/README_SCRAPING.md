# InfoHub RAG System - Web Scraping Setup

–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ scraping –¥–ª—è –Ω–∞–ø–æ–ª–Ω–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.

## üìã –û–±–∑–æ—Ä

–°–∏—Å—Ç–µ–º–∞ scraping –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∞ –¥–ª—è:
- –°–±–æ—Ä–∞ –Ω–∞–ª–æ–≥–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å infohub.rs.ge
- –û–±—Ä–∞–±–æ—Ç–∫–∏ –∏ —Å–æ–∑–¥–∞–Ω–∏—è embeddings
- –ó–∞–≥—Ä—É–∑–∫–∏ –≤ ChromaDB –¥–ª—è RAG
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫ (–ª–æ–∫–∞–ª—å–Ω–æ)

–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π scrape –Ω–∞ 5 —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö:

```bash
cd backend
python scripts/populate_vector_db.py --max-pages 5 --initial-run
```

–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:

```bash
# –ü–æ–∫–∞–∑–∞—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ scraper
python scripts/populate_vector_db.py --show-state

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ vector store
python -c "from rag.vector_store import vector_store; print(f'Documents: {vector_store.get_count()}')"
```

### 2. –†–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏–µ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ (Hetzner)

#### SSH –∫ —Å–µ—Ä–≤–µ—Ä—É

```bash
ssh root@46.224.145.5
cd /root/infohub
```

#### –ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Å–∫—Ä–∏–ø—Ç—ã –Ω–∞ —Å–µ—Ä–≤–µ—Ä

```bash
# –ù–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–∞—à–∏–Ω–µ (Windows)
scp backend/scripts/populate_vector_db.py root@46.224.145.5:/root/infohub/backend/scripts/
scp backend/scripts/setup_cron.sh root@46.224.145.5:/root/infohub/backend/scripts/
```

–ò–ª–∏ —á–µ—Ä–µ–∑ Git:

```bash
# –ù–∞ —Å–µ—Ä–≤–µ—Ä–µ
cd /root/infohub
git pull origin main
```

#### –ù–∞—Å—Ç—Ä–æ–∏—Ç—å cron job

```bash
# –ù–∞ —Å–µ—Ä–≤–µ—Ä–µ
cd /root/infohub/backend/scripts
chmod +x setup_cron.sh
./setup_cron.sh
```

–≠—Ç–æ —Å–æ–∑–¥–∞—Å—Ç:
- `/root/infohub/run_scraper.sh` - –æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ scraper
- Cron job, –∑–∞–ø—É—Å–∫–∞—é—â–∏–π—Å—è –∫–∞–∂–¥—ã–π –¥–µ–Ω—å –≤ 3:00 AM
- –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—é `/root/infohub/logs/` –¥–ª—è –ª–æ–≥–æ–≤

#### –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ

```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Ä—É—á–Ω—É—é
/root/infohub/run_scraper.sh

# –ò–ª–∏ —á–µ—Ä–µ–∑ docker exec –Ω–∞–ø—Ä—è–º—É—é
docker exec infohub-backend-1 python /app/scripts/populate_vector_db.py --max-pages 10 --initial-run
```

## üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã scraping

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–∫—Ä–∏–ø—Ç–∞

```bash
python scripts/populate_vector_db.py [OPTIONS]
```

–û–ø—Ü–∏–∏:
- `--start-url URL` - –Ω–∞—á–∞–ª—å–Ω—ã–π URL (default: https://infohub.rs.ge/ka)
- `--max-pages N` - –º–∞–∫—Å. —Å—Ç—Ä–∞–Ω–∏—Ü –∑–∞ –∑–∞–ø—É—Å–∫ (default: 50)
- `--max-depth N` - –º–∞–∫—Å. –≥–ª—É–±–∏–Ω–∞ —Å—Å—ã–ª–æ–∫ (default: 2)
- `--initial-run` - –Ω–∞—á–∞—Ç—å —Å –Ω—É–ª—è, –∏–≥–Ω–æ—Ä–∏—Ä—É—è —Å–æ—Å—Ç–æ—è–Ω–∏–µ
- `--show-state` - –ø–æ–∫–∞–∑–∞—Ç—å —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏ –≤—ã–π—Ç–∏

### –ü—Ä–∏–º–µ—Ä—ã

```bash
# –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ - 10 —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Ç–µ—Å—Ç–∞
python scripts/populate_vector_db.py --max-pages 10 --initial-run

# –ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π incremental - 50 —Å—Ç—Ä–∞–Ω–∏—Ü
python scripts/populate_vector_db.py --max-pages 50

# –ü–æ–∫–∞–∑–∞—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ
python scripts/populate_vector_db.py --show-state

# –ë–æ–ª—å—à–æ–π batch - 200 —Å—Ç—Ä–∞–Ω–∏—Ü
python scripts/populate_vector_db.py --max-pages 200
```

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ scraper

```bash
# –ü–æ–∫–∞–∑–∞—Ç—å JSON —Å —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º
python scripts/populate_vector_db.py --show-state

# –ò–ª–∏ –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª –Ω–∞–ø—Ä—è–º—É—é
cat backend/data/scraper_state.json
```

–°–æ—Å—Ç–æ—è–Ω–∏–µ –≤–∫–ª—é—á–∞–µ—Ç:
- `visited_urls` - —Å–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL
- `last_run` - –¥–∞—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—É—Å–∫–∞
- `total_documents` - –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–æ–±—Ä–∞–Ω–æ
- `total_pages_scraped` - –≤—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ
- `runs` - –∏—Å—Ç–æ—Ä–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 30 –∑–∞–ø—É—Å–∫–æ–≤

### –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ

```bash
# –õ–æ–≥–∏ cron
tail -f /root/infohub/logs/cron.log

# –õ–æ–≥–∏ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤
ls -lh /root/infohub/logs/scraper_*.log
tail -f /root/infohub/logs/scraper_$(ls -t /root/infohub/logs/scraper_*.log | head -1)

# –õ–æ–≥–∏ –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
docker exec infohub-backend-1 tail -f /app/logs/scraper.log
```

### –ü—Ä–æ–≤–µ—Ä–∏—Ç—å vector store

```bash
# –ß–µ—Ä–µ–∑ docker exec
docker exec infohub-backend-1 python -c "from rag.vector_store import vector_store; print(f'Documents in ChromaDB: {vector_store.get_count()}')"

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á–µ—Ä–µ–∑ API
curl https://tax-advisor.ge/api/v1/public/query \
  -X POST \
  -H "Content-Type: application/json" \
  -d '{"query":"·É†·Éê ·Éê·É†·Éò·É° ·Éì·É¶·Éí?", "language":"ka"}'
```

### –ü—Ä–æ–≤–µ—Ä–∏—Ç—å cron job

```bash
# –ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ–∫—É—â–∏–µ cron jobs
crontab -l

# –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å cron jobs
crontab -e
```

## üõ†Ô∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ rate limiting

–ò–∑–º–µ–Ω–∏—Ç—å rate limiting –≤ `.env`:

```bash
# –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–µ–∫—É–Ω–¥—ã)
SCRAPER_DELAY=3.0  # —É–≤–µ–ª–∏—á–∏—Ç—å –µ—Å–ª–∏ –±–ª–æ–∫–∏—Ä—É—é—Ç

# –£–≤–∞–∂–∞—Ç—å robots.txt
SCRAPER_RESPECT_ROBOTS_TXT=true

# User agent
SCRAPER_USER_AGENT=InfoHubAI-Bot/1.0
```

–ò–ª–∏ –∏–∑–º–µ–Ω–∏—Ç—å –≤ `run_scraper.sh`:

```bash
MAX_PAGES=50  # —É–º–µ–Ω—å—à–∏—Ç—å –¥–ª—è –±–æ–ª–µ–µ –º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ scraping
```

## üìà –°—Ç—Ä–∞—Ç–µ–≥–∏—è scraping

### Incremental approach

1. **–î–µ–Ω—å 1**: –ó–∞–ø—É—Å–∫ —Å `--initial-run`, –æ–±—Ä–∞–±–æ—Ç–∫–∞ 50 —Å—Ç—Ä–∞–Ω–∏—Ü
2. **–î–µ–Ω—å 2-N**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ cron, 50 –Ω–æ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞–∂–¥—ã–π –¥–µ–Ω—å
3. **State tracking**: –°–∫—Ä–∏–ø—Ç –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ URL, –Ω–µ –¥—É–±–ª–∏—Ä—É–µ—Ç
4. **Rate limiting**: 2-5 —Å–µ–∫ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –≤ `.env`)

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

- **–ù–∞—á–∞—Ç—å –º–µ–¥–ª–µ–Ω–Ω–æ**: 10-20 —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Ç–µ—Å—Ç–∞
- **–ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å**: –ü—Ä–æ–≤–µ—Ä—è—Ç—å –ª–æ–≥–∏ –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π
- **–£–≤–µ–ª–∏—á–∏–≤–∞—Ç—å –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ**: –ï—Å–ª–∏ –≤—Å—ë –û–ö, —É–≤–µ–ª–∏—á–∏—Ç—å –¥–æ 50-100 —Å—Ç—Ä–∞–Ω–∏—Ü/–¥–µ–Ω—å
- **–ü—Ä–æ–≤–µ—Ä—è—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ**: –£–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã

## üêõ Troubleshooting

### Scraper –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
docker exec infohub-backend-1 python -c "import aiohttp, bs4, sqlalchemy; print('OK')"

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ infohub.rs.ge
curl -I https://infohub.rs.ge/ka

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å robots.txt
curl https://infohub.rs.ge/robots.txt
```

### ChromaDB –Ω–µ –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä ChromaDB
docker ps | grep chroma

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ .env
docker exec infohub-backend-1 env | grep CHROMA

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
docker exec infohub-backend-1 python -c "from rag.vector_store import vector_store; print(vector_store.client)"
```

### –ù–µ—Ç embeddings

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –º–æ–¥–µ–ª—å
docker exec infohub-backend-1 python -c "from rag.embeddings import embeddings_generator; print(embeddings_generator.model)"

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å GPU/CPU
docker exec infohub-backend-1 nvidia-smi  # if GPU
docker exec infohub-backend-1 python -c "import torch; print(torch.cuda.is_available())"
```

### Cron –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å cron service
systemctl status cron

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–∞–≤–∞
ls -l /root/infohub/run_scraper.sh

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏ cron
grep CRON /var/log/syslog
tail -f /root/infohub/logs/cron.log
```

## üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ

### –û–±–Ω–æ–≤–∏—Ç—å –∫–æ–¥ scraper

```bash
# –ù–∞ —Å–µ—Ä–≤–µ—Ä–µ
cd /root/infohub
git pull origin main

# –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å backend
docker-compose up -d --force-recreate backend
```

### –°–±—Ä–æ—Å–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ scraper

```bash
# –ù–∞ —Å–µ—Ä–≤–µ—Ä–µ
rm -f /root/infohub/backend/data/scraper_state.json

# –ò–ª–∏ —á–µ—Ä–µ–∑ docker
docker exec infohub-backend-1 rm -f /app/data/scraper_state.json
```

## üìù –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤

```
backend/
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ populate_vector_db.py    # –ì–ª–∞–≤–Ω—ã–π —Å–∫—Ä–∏–ø—Ç scraping
‚îÇ   ‚îú‚îÄ‚îÄ setup_cron.sh             # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ cron job
‚îÇ   ‚îî‚îÄ‚îÄ README_SCRAPING.md        # –≠—Ç–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ scraper_state.json        # –°–æ—Å—Ç–æ—è–Ω–∏–µ scraper (auto-generated)
‚îú‚îÄ‚îÄ scraper/
‚îÇ   ‚îú‚îÄ‚îÄ base_scraper.py           # –ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å —Å rate limiting
‚îÇ   ‚îî‚îÄ‚îÄ infohub_scraper.py        # InfoHub-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π scraper
‚îî‚îÄ‚îÄ logs/
    ‚îî‚îÄ‚îÄ scraper.log               # –õ–æ–≥–∏ scraping

–ù–∞ —Å–µ—Ä–≤–µ—Ä–µ:
/root/infohub/
‚îú‚îÄ‚îÄ run_scraper.sh                # –û–±—ë—Ä—Ç–∫–∞ –¥–ª—è cron
‚îî‚îÄ‚îÄ logs/
    ‚îú‚îÄ‚îÄ cron.log                  # –õ–æ–≥–∏ cron
    ‚îî‚îÄ‚îÄ scraper_YYYYMMDD_HHMMSS.log  # –õ–æ–≥–∏ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤
```

## üéØ Next Steps

1. ‚úÖ –û–±–Ω–æ–≤–∏—Ç—å base_url –≤ InfoHubScraper ‚Üí `https://infohub.rs.ge`
2. ‚úÖ –°–æ–∑–¥–∞—Ç—å —Å–∫—Ä–∏–ø—Ç `populate_vector_db.py` —Å state tracking
3. ‚úÖ –°–æ–∑–¥–∞—Ç—å `setup_cron.sh` –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏
4. üîÑ –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ 5-10 —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö
5. üîÑ –†–∞–∑–≤–µ—Ä–Ω—É—Ç—å –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ
6. üîÑ –ù–∞—Å—Ç—Ä–æ–∏—Ç—å cron job
7. üîÑ –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π
8. üîÑ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ RAG –æ—Ç–≤–µ—Ç–æ–≤
